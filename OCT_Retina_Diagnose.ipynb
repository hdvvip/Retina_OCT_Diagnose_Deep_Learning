{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCT Retina Diagnose\n",
    "\n",
    "## Project: Build CNN Model to diagnose OCT Retina images\n",
    "\n",
    "---\n",
    "\n",
    "### Why We're Here \n",
    "\n",
    "Retinal optical coherence tomography (OCT) is an imaging technique used to capture high-resolution cross sections of the retinas of living patients. Approximately 30 million OCT scans are performed each year, and the analysis and interpretation of these images takes up a significant amount of time (Swanson and Fujimoto, 2017).\n",
    "\n",
    "![](https://i.imgur.com/fSTeZMd.png)\n",
    "Figure 1: Representative Optical Coherence Tomography Images and the Workflow Diagram [Kermany et. al. 2018]\n",
    "\n",
    "(A) (Far left) choroidal neovascularization (CNV) with neovascular membrane (white arrowheads) and associated subretinal fluid (arrows). (Middle left) Diabetic macular edema (DME) with retinal-thickening-associated intraretinal fluid (arrows). (Middle right) Multiple drusen (arrowheads) present in early AMD. (Far right) Normal retina with preserved foveal contour and absence of any retinal fluid/edema.\n",
    "\n",
    "#### Content\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (NORMAL,CNV,DME,DRUSEN). There are 84,495 X-Ray images (JPEG) and 4 categories (NORMAL,CNV,DME,DRUSEN).\n",
    "\n",
    "Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.\n",
    "\n",
    "Optical coherence tomography (OCT) images (Spectralis OCT, Heidelberg Engineering, Germany) were selected from retrospective cohorts of adult patients from the Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Center Ophthalmology Associates, the Shanghai First Peopleâ€™s Hospital, and Beijing Tongren Eye Center between July 1, 2013 and March 1, 2017.\n",
    "\n",
    "Before training, each image went through a tiered grading system consisting of multiple layers of trained graders of increasing exper- tise for verification and correction of image labels. Each image imported into the database started with a label matching the most recent diagnosis of the patient. The first tier of graders consisted of undergraduate and medical students who had taken and passed an OCT interpretation course review. This first tier of graders conducted initial quality control and excluded OCT images containing severe artifacts or significant image resolution reductions. The second tier of graders consisted of four ophthalmologists who independently graded each image that had passed the first tier. The presence or absence of choroidal neovascularization (active or in the form of subretinal fibrosis), macular edema, drusen, and other pathologies visible on the OCT scan were recorded. Finally, a third tier of two senior independent retinal specialists, each with over 20 years of clinical retina experience, verified the true labels for each image. The dataset selection and stratification process is displayed in a CONSORT-style diagram in Figure 2B. To account for human error in grading, a validation subset of 993 scans was graded separately by two ophthalmologist graders, with disagreement in clinical labels arbitrated by a senior retinal specialist.\n",
    "\n",
    "\n",
    "#### In this notebook we will design and benchmark CNN models in accuracy of diagnosing CNV, DME, DRUSEN, and NORMAL Retina OCT image\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Define Train and Test function\n",
    "* [Step 2](#step2): Train and Test origin ResNet18 \n",
    "* [Step 3](#step3): Train and Test modified ResNet18 (build CNN from scratch)\n",
    "* [Step 4](#step4): Train and Test pretrained ResNet50 (transfer learning)\n",
    "* [Step 5](#step5): Conclusion\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "Download Dataset OCT2017.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = './OCT_data'\n",
    "train_dir = data_dir + '/train224'\n",
    "val_dir = data_dir + '/val224'\n",
    "test_dir = data_dir + '/test224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for data\n",
    "# Because the original image already stay in from 224x224\n",
    "# No need to do resize or crop\n",
    "\n",
    "# Data Augmentation:\n",
    "# HorizontalFlip is reasonable enough\n",
    "# Not anthing else\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "data_transforms = [train_transform, val_transform, test_transform]\n",
    "\n",
    "# Load datasets\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "image_datasets = [train_data, val_data, test_data]\n",
    "\n",
    "# Create Loader\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "loaders = {\"train\":trainloader, \"test\":testloader, \"val\":valloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names of Class Labels mapped by ImageFolder Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_to_name = train_data.class_to_idx\n",
    "cat_to_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1668"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check balance of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1000\n",
       "1    1000\n",
       "2    1000\n",
       "0    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "train_info_df = pd.read_csv('OCT_data/train.csv')\n",
    "train_info_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    100\n",
       "2    100\n",
       "1    100\n",
       "0    100\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "val_info_df = pd.read_csv('OCT_data/val.csv')\n",
    "val_info_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    242\n",
       "2    242\n",
       "1    242\n",
       "0    242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "test_info_df = pd.read_csv('OCT_data/test.csv')\n",
    "test_info_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Define Train and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, save_path=None, valid_loss_min=np.Inf):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ## record the average training loss, using something like\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            print_every = batch_idx\n",
    "            \n",
    "            # Get val loss every 400 batch\n",
    "            if print_every % 400 == 0:\n",
    "                \n",
    "                ######################    \n",
    "                # validate the model #\n",
    "                ######################\n",
    "                model.eval()\n",
    "                for batch_idx, (data, target) in enumerate(loaders['val']):\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    ## update the average validation loss\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "            \n",
    "                # print training/validation statistics\n",
    "                print('Epoch: {} \\tbatch {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                    epoch,\n",
    "                    print_every,\n",
    "                    train_loss,\n",
    "                    valid_loss\n",
    "                    ))\n",
    "                # Set model training again\n",
    "                model.train()\n",
    "                \n",
    "        \n",
    "                ##  save the model if validation loss has decreased\n",
    "                if (save_path is not None):\n",
    "                    if (valid_loss <= valid_loss_min):\n",
    "                        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                        valid_loss_min,\n",
    "                        valid_loss))\n",
    "                        torch.save(model.state_dict(), save_path)\n",
    "                        valid_loss_min = valid_loss\n",
    "        \n",
    "            \n",
    "    # return trained model\n",
    "    return model, valid_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model):\n",
    "    acc_test = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            Z = model(data)\n",
    "            target_pred = Z.data.max(dim=1)[1]\n",
    "            acc_test += torch.sum(target_pred==target).item()\n",
    "    \n",
    "    \n",
    "    return acc_test/len(loaders['test'].dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Train and Test Origin ResNet-18\n",
    "\n",
    "<img src='https://www.researchgate.net/profile/Paolo_Napoletano/publication/322476121/figure/tbl1/AS:668726449946625@1536448218498/ResNet-18-Architecture.png' width=500px>\n",
    "\n",
    "In here, we will build a general ResNet model.\n",
    "To get specific ResNet we only need to define number of convolution layers in each convolution block.\n",
    "\n",
    "For ex:\n",
    "- resnet18 = ResNet(2,2,2,2,N_output)\n",
    "- resnet34 = ResNet(3,4,6,3,N_output)\n",
    "\n",
    "Instead of importing prebuilt ResNet-18 model from pytroch, we decided to implement by hand again to warm up our PyTorch skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv1_stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=conv1_stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.bn(self.conv1(x))\n",
    "        y = F.relu(y)\n",
    "        y = self.bn(self.conv2(y))\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, N):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.basic_block1 = BasicBlock(in_channels, out_channels, conv1_stride=2)\n",
    "        self.basic_blocks = get_clones(BasicBlock(out_channels, out_channels, conv1_stride=1), self.N-1)\n",
    "        self.downsample = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n",
    "                                        nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add residual skip after pass each block\n",
    "        y = self.basic_block1(x)\n",
    "        y = y + self.downsample(x)\n",
    "\n",
    "        for i in range(self.N-1):\n",
    "            y = y + self.basic_blocks[i](y)\n",
    "            \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, N1, N2, N3, N4, N_output):\n",
    "        super().__init__()\n",
    "        self.N1 = N1\n",
    "        self.N2 = N2\n",
    "        self.N3 = N3\n",
    "        self.N4 = N4\n",
    "        self.N_output = N_output\n",
    "        \n",
    "        # Layer 0\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.basic_blocks_1 = get_clones(BasicBlock(64, 64, conv1_stride=1), self.N1)\n",
    "        \n",
    "        \n",
    "        # Other layers\n",
    "        self.layer2 = Layer(64, 128, self.N2)\n",
    "        self.layer3 = Layer(128, 256, self.N3)\n",
    "        self.layer4 = Layer(256, 512, self.N4)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Linear(512, self.N_output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 0\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.pool1(y)\n",
    "        \n",
    "        # Layer 1\n",
    "        for i in range(self.N1):\n",
    "            y = y + self.basic_blocks_1[i](y)\n",
    "        \n",
    "        y = self.layer2(y)\n",
    "        y = self.layer3(y)\n",
    "        y = self.layer4(y)\n",
    "        \n",
    "        y = self.avgpool(y)\n",
    "        y = y.view(x.size(0), -1) # Flatten\n",
    "        \n",
    "        y = self.fc(y)\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate resnet18 model, loss function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (basic_blocks_1): ModuleList(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Layer(\n",
       "    (basic_block1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (basic_blocks): ModuleList(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Layer(\n",
       "    (basic_block1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (basic_blocks): ModuleList(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Layer(\n",
       "    (basic_block1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (basic_blocks): ModuleList(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate the model\n",
    "resnet18_model = ResNet(2,2,2,2,4)\n",
    "\n",
    "# Loss function fomula\n",
    "res18_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reference from https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
    "res18_optimizer = optim.SGD(resnet18_model.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "resnet18_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min_res18 = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tbatch 0 \tTraining Loss: 1.386990 \tValidation Loss: 1.388235\n",
      "Epoch: 1 \tbatch 400 \tTraining Loss: 0.599900 \tValidation Loss: 1.376849\n",
      "Epoch: 1 \tbatch 800 \tTraining Loss: 0.479702 \tValidation Loss: 1.243111\n",
      "Epoch: 1 \tbatch 1200 \tTraining Loss: 0.420820 \tValidation Loss: 1.136152\n",
      "Epoch: 1 \tbatch 1600 \tTraining Loss: 0.382876 \tValidation Loss: 1.203773\n",
      "Epoch: 2 \tbatch 0 \tTraining Loss: 0.388344 \tValidation Loss: 1.171317\n",
      "Epoch: 2 \tbatch 400 \tTraining Loss: 0.245765 \tValidation Loss: 1.261480\n",
      "Epoch: 2 \tbatch 800 \tTraining Loss: 0.237673 \tValidation Loss: 1.097593\n",
      "Epoch: 2 \tbatch 1200 \tTraining Loss: 0.229866 \tValidation Loss: 1.124406\n",
      "Epoch: 2 \tbatch 1600 \tTraining Loss: 0.223209 \tValidation Loss: 1.053322\n",
      "Epoch: 3 \tbatch 0 \tTraining Loss: 0.116971 \tValidation Loss: 1.301796\n",
      "Epoch: 3 \tbatch 400 \tTraining Loss: 0.189956 \tValidation Loss: 1.121692\n",
      "Epoch: 3 \tbatch 800 \tTraining Loss: 0.182260 \tValidation Loss: 1.160074\n",
      "Epoch: 3 \tbatch 1200 \tTraining Loss: 0.178916 \tValidation Loss: 1.268536\n",
      "Epoch: 3 \tbatch 1600 \tTraining Loss: 0.176010 \tValidation Loss: 1.335797\n"
     ]
    }
   ],
   "source": [
    "resnet18_model, val_loss_min_res18 = train(3, loaders, resnet18_model, res18_optimizer, res18_criterion,\n",
    "                                          save_path=None,\n",
    "                                          valid_loss_min=val_loss_min_res18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29545454545454547"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(loaders, resnet18_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Train and Test modified ResNet18\n",
    "Instead build CNN from scratch, I will modify ResNet to achieve higher accuracy\n",
    "\n",
    "ResNet with GroupNorm with num_group=1 instead of BatchNorm\n",
    "--> LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv1_stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=conv1_stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.gn = nn.GroupNorm(1, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.gn(self.conv1(x))\n",
    "        y = F.relu(y)\n",
    "        y = self.gn(self.conv2(y))\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, N):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.basic_block1 = BasicBlock(in_channels, out_channels, conv1_stride=2)\n",
    "        self.basic_blocks = get_clones(BasicBlock(out_channels, out_channels, conv1_stride=1), self.N-1)\n",
    "        self.downsample = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n",
    "                                        nn.GroupNorm(1, out_channels))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add residual skip after pass each block\n",
    "        y = self.basic_block1(x)\n",
    "        y = y + self.downsample(x)\n",
    "\n",
    "        for i in range(self.N-1):\n",
    "            y = y + self.basic_blocks[i](y)\n",
    "            \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetGN(nn.Module):\n",
    "    def __init__(self, N1, N2, N3, N4, N_output):\n",
    "        super().__init__()\n",
    "        self.N1 = N1\n",
    "        self.N2 = N2\n",
    "        self.N3 = N3\n",
    "        self.N4 = N4\n",
    "        self.N_output = N_output\n",
    "        \n",
    "        # Layer 0\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.gn1 = nn.GroupNorm(1, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.basic_blocks_1 = get_clones(BasicBlock(64, 64, conv1_stride=1), self.N1)\n",
    "        \n",
    "        \n",
    "        # Other layers\n",
    "        self.layer2 = Layer(64, 128, self.N2)\n",
    "        self.layer3 = Layer(128, 256, self.N3)\n",
    "        self.layer4 = Layer(256, 512, self.N4)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Linear(512, self.N_output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 0\n",
    "        y = F.relu(self.gn1(self.conv1(x)))\n",
    "        y = self.pool1(y)\n",
    "        \n",
    "        # Layer 1\n",
    "        for i in range(self.N1):\n",
    "            y = y + self.basic_blocks_1[i](y)\n",
    "        \n",
    "        y = self.layer2(y)\n",
    "        y = self.layer3(y)\n",
    "        y = self.layer4(y)\n",
    "        \n",
    "        y = self.avgpool(y)\n",
    "        y = y.view(x.size(0), -1) # Flatten\n",
    "        \n",
    "        y = self.fc(y)\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetGN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (gn1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (basic_blocks_1): ModuleList(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (gn): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (gn): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Layer(\n",
       "    (basic_block1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (gn): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (basic_blocks): ModuleList(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Layer(\n",
       "    (basic_block1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (gn): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (basic_blocks): ModuleList(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Layer(\n",
       "    (basic_block1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (gn): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (basic_blocks): ModuleList(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate the model\n",
    "resnet18_GN_model = ResNetGN(2,2,2,2,4)\n",
    "\n",
    "# Loss function fomula\n",
    "res18_GN_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reference from https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
    "res18_GN_optimizer = optim.SGD(resnet18_GN_model.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "resnet18_GN_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min_res18_gn = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tbatch 0 \tTraining Loss: 1.633554 \tValidation Loss: 3.744724\n",
      "Epoch: 1 \tbatch 400 \tTraining Loss: 1.365723 \tValidation Loss: 1.557902\n",
      "Epoch: 1 \tbatch 800 \tTraining Loss: 1.275613 \tValidation Loss: 1.046402\n",
      "Epoch: 1 \tbatch 1200 \tTraining Loss: 1.116997 \tValidation Loss: 1.022070\n",
      "Epoch: 1 \tbatch 1600 \tTraining Loss: 0.998848 \tValidation Loss: 0.649461\n",
      "Epoch: 2 \tbatch 0 \tTraining Loss: 0.445842 \tValidation Loss: 0.730278\n",
      "Epoch: 2 \tbatch 400 \tTraining Loss: 0.494867 \tValidation Loss: 0.425684\n",
      "Epoch: 2 \tbatch 800 \tTraining Loss: 0.452669 \tValidation Loss: 0.329882\n",
      "Epoch: 2 \tbatch 1200 \tTraining Loss: 0.416345 \tValidation Loss: 0.290522\n",
      "Epoch: 2 \tbatch 1600 \tTraining Loss: 0.390313 \tValidation Loss: 0.201673\n",
      "Epoch: 3 \tbatch 0 \tTraining Loss: 0.128026 \tValidation Loss: 0.144296\n",
      "Epoch: 3 \tbatch 400 \tTraining Loss: 0.280475 \tValidation Loss: 0.126784\n",
      "Epoch: 3 \tbatch 800 \tTraining Loss: 0.272431 \tValidation Loss: 0.229600\n",
      "Epoch: 3 \tbatch 1200 \tTraining Loss: 0.262124 \tValidation Loss: 0.110513\n",
      "Epoch: 3 \tbatch 1600 \tTraining Loss: 0.250603 \tValidation Loss: 0.127543\n"
     ]
    }
   ],
   "source": [
    "resnet18_GN_model, val_loss_min_res18_gn = train(3, loaders, resnet18_GN_model, res18_GN_optimizer, res18_GN_criterion,\n",
    "                                          save_path=None,\n",
    "                                          valid_loss_min=val_loss_min_res18_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981404958677686"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(loaders, resnet18_GN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: Insanely high accuracy from a model that is trained from scratch__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Train and Test pretrained ResNet50 (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_pretrained = models.resnet50(pretrained=True)\n",
    "# model_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze all the pretrained weight\n",
    "for params in model_pretrained.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# Define new classifer\n",
    "fc_input_features = model_pretrained.fc.in_features\n",
    "model_pretrained.fc = nn.Linear(fc_input_features, 4)\n",
    "\n",
    "# Loss function fomula\n",
    "model_pretrained_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reference from https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
    "model_pretrained_optimizer = optim.SGD(model_pretrained.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "model_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min_model_pretrained = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tbatch 0 \tloss: 1.4611423015594482\n",
      "Epoch 1 \tbatch 100 \tloss: 1.0470964908599854\n",
      "Epoch 1 \tbatch 200 \tloss: 0.9074360728263855\n",
      "Epoch 1 \tbatch 300 \tloss: 0.8397747874259949\n",
      "Epoch 1 \tbatch 400 \tloss: 0.7934359908103943\n",
      "Epoch 1 \tbatch 500 \tloss: 0.7630904316902161\n",
      "Epoch 1 \tbatch 600 \tloss: 0.7393508553504944\n",
      "Epoch 1 \tbatch 700 \tloss: 0.7205396890640259\n",
      "Epoch 1 \tbatch 800 \tloss: 0.702239990234375\n",
      "Epoch 1 \tbatch 900 \tloss: 0.6898284554481506\n",
      "Epoch 1 \tbatch 1000 \tloss: 0.6785690784454346\n",
      "Epoch 1 \tbatch 1100 \tloss: 0.669047474861145\n",
      "Epoch 1 \tbatch 1200 \tloss: 0.6647937297821045\n",
      "Epoch 1 \tbatch 1300 \tloss: 0.6556822061538696\n",
      "Epoch 1 \tbatch 1400 \tloss: 0.6490129828453064\n",
      "Epoch 1 \tbatch 1500 \tloss: 0.642238438129425\n",
      "Epoch 1 \tbatch 1600 \tloss: 0.6345715522766113\n",
      "Epoch: 1 \tTraining Loss: 0.631112 \tValidation Loss: 0.438402\n",
      "Epoch 2 \tbatch 0 \tloss: 0.3330639898777008\n",
      "Epoch 2 \tbatch 100 \tloss: 0.5132921934127808\n",
      "Epoch 2 \tbatch 200 \tloss: 0.5142815113067627\n",
      "Epoch 2 \tbatch 300 \tloss: 0.5171827673912048\n",
      "Epoch 2 \tbatch 400 \tloss: 0.5200703740119934\n",
      "Epoch 2 \tbatch 500 \tloss: 0.5186693668365479\n",
      "Epoch 2 \tbatch 600 \tloss: 0.518456220626831\n",
      "Epoch 2 \tbatch 700 \tloss: 0.5215460062026978\n",
      "Epoch 2 \tbatch 800 \tloss: 0.5190197825431824\n",
      "Epoch 2 \tbatch 900 \tloss: 0.5208566784858704\n",
      "Epoch 2 \tbatch 1000 \tloss: 0.5209953784942627\n",
      "Epoch 2 \tbatch 1100 \tloss: 0.5215597748756409\n",
      "Epoch 2 \tbatch 1200 \tloss: 0.5222610235214233\n",
      "Epoch 2 \tbatch 1300 \tloss: 0.5224524140357971\n",
      "Epoch 2 \tbatch 1400 \tloss: 0.5202474594116211\n",
      "Epoch 2 \tbatch 1500 \tloss: 0.5210102796554565\n",
      "Epoch 2 \tbatch 1600 \tloss: 0.5211495161056519\n",
      "Epoch: 2 \tTraining Loss: 0.520748 \tValidation Loss: 0.418137\n",
      "Epoch 3 \tbatch 0 \tloss: 0.34286630153656006\n",
      "Epoch 3 \tbatch 100 \tloss: 0.5030242800712585\n",
      "Epoch 3 \tbatch 200 \tloss: 0.48949846625328064\n",
      "Epoch 3 \tbatch 300 \tloss: 0.4998074173927307\n",
      "Epoch 3 \tbatch 400 \tloss: 0.4975091218948364\n",
      "Epoch 3 \tbatch 500 \tloss: 0.49577510356903076\n",
      "Epoch 3 \tbatch 600 \tloss: 0.4977800250053406\n",
      "Epoch 3 \tbatch 700 \tloss: 0.4941228926181793\n",
      "Epoch 3 \tbatch 800 \tloss: 0.49503007531166077\n",
      "Epoch 3 \tbatch 900 \tloss: 0.4947779178619385\n",
      "Epoch 3 \tbatch 1000 \tloss: 0.49672606587409973\n",
      "Epoch 3 \tbatch 1100 \tloss: 0.49741607904434204\n",
      "Epoch 3 \tbatch 1200 \tloss: 0.5002112984657288\n",
      "Epoch 3 \tbatch 1300 \tloss: 0.4997541904449463\n",
      "Epoch 3 \tbatch 1400 \tloss: 0.5009052157402039\n",
      "Epoch 3 \tbatch 1500 \tloss: 0.5017880201339722\n",
      "Epoch 3 \tbatch 1600 \tloss: 0.500693678855896\n",
      "Epoch: 3 \tTraining Loss: 0.499531 \tValidation Loss: 0.359317\n"
     ]
    }
   ],
   "source": [
    "model_pretrained, val_loss_min_model_pretrained = train(3, loaders, model_pretrained, model_pretrained_optimizer, \n",
    "                                                        model_pretrained_criterion,\n",
    "                                                        valid_loss_min=val_loss_min_model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8646694214876033"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(loaders, model_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. With epoch = 3, we trained 3 models:\n",
    "    - Orginal Resnet-18, accuracy = 0.29\n",
    "    - Modified Resnet-18, GroupNorm(group=1) --> LayerNorm, accuracy = 0.98\n",
    "    - Pretrained Resnet-50, accuracy = 0.86\n",
    "2. SGD perform better than Adam optimizer\n",
    "3. Interestingly, when build CNN from scratch by modifying BatchNorm to GroupNorm --> we achieve insanely high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
